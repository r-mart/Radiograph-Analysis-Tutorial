{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# print(f\"Install in: {sys.executable}\")\n",
    "# !{sys.executable} -m pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To debug external functions\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-21T11:21:32.308250Z",
     "iopub.status.busy": "2021-05-21T11:21:32.307893Z",
     "iopub.status.idle": "2021-05-21T11:21:33.631675Z",
     "shell.execute_reply": "2021-05-21T11:21:33.630819Z",
     "shell.execute_reply.started": "2021-05-21T11:21:32.308211Z"
    }
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "import gc\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import cv2\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm_notebook as tqdm # progress bar\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Albumenatations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing from src.local package\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "from src.data.datasets import DetectionDataset\n",
    "from src.models.detection import DetectionBaseline\n",
    "from src.train.utils import boxes_xyxy_rel_to_abs, boxes_xyxy_abs_to_rel\n",
    "from src.train.DetectionTrainer import DetectionTrainer\n",
    "from src.train.metrics import calculate_mAP\n",
    "from src.visualization.images import show_img, show_img_with_boxes\n",
    "from src.visualization.metrics import plot_confusion_matrix\n",
    "from src.visualization.utils import cxcywh_to_xyxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-21T11:21:33.692018Z",
     "iopub.status.busy": "2021-05-21T11:21:33.691583Z",
     "iopub.status.idle": "2021-05-21T11:21:33.702230Z",
     "shell.execute_reply": "2021-05-21T11:21:33.701424Z",
     "shell.execute_reply.started": "2021-05-21T11:21:33.691982Z"
    }
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    \"\"\" seed random number generators to make runs deterministic \"\"\"\n",
    "    \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    \n",
    "class Config:\n",
    "    \"\"\" Configuration for the training \"\"\"\n",
    "    \n",
    "    time_stamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    exp_name = f\"ssd_{time_stamp}\" \n",
    "    log_path = Path(\"./logs\") / \"detection\" / exp_name    \n",
    "    fold_num: int = 0            \n",
    "    seed: int = 2021\n",
    "    num_classes: int = 2     \n",
    "    freeze_backbone = False # don't train backbone weights if true\n",
    "    aspect_ratios: list = [1., 2., 0.5] # anchor box aspect ratios per cell\n",
    "    img_size: int = (768,) * 2 # implementation expects height = width  \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # computing device    \n",
    "    num_workers: int = 8 # number of processors used to prepare the batches\n",
    "    batch_size: int = 16\n",
    "    n_epochs: int = 200\n",
    "    lr: float = 1e-4\n",
    "    use_scheduler = False\n",
    "    label_dict = {\n",
    "        0 : \"negative\",\n",
    "        1 : \"positive\"\n",
    "    }\n",
    "#     label_dict = {\n",
    "#         0 : \"negative\", \n",
    "#         1 : \"typical\", \n",
    "#         2 : \"indeterminate\", \n",
    "#         3 : \"atypical\"\n",
    "#     }     \n",
    "\n",
    "seed_everything(Config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_batch(images, gt_targets, image_ids, pred_targets=None):\n",
    "    \"\"\" plots image batch returned from dataloader and optionally predictions\n",
    "    \n",
    "    Expects data to be detached from gradients and moved to CPU\n",
    "    \"\"\"\n",
    "    \n",
    "    # for visibility, plot only a few images for large batches\n",
    "    n_plot_max = 8\n",
    "    if len(images) > n_plot_max:\n",
    "        n_plot = n_plot_max        \n",
    "        sample_idx = torch.randperm(len(images))[:n_plot]                \n",
    "        images = [images[i] for i in sample_idx]\n",
    "        gt_targets = [gt_targets[i] for i in sample_idx]             \n",
    "        image_ids = [image_ids[i.item()] for i in sample_idx]\n",
    "        if pred_targets is not None:            \n",
    "            pred_targets = [pred_targets[i] for i in sample_idx]  \n",
    "    else:\n",
    "        n_plot = len(images)\n",
    "    \n",
    "    n_cols=2\n",
    "    n_rows=n_plot // 2    \n",
    "    \n",
    "    figsize= (n_cols * 7, n_rows * 7)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 14),  nrows=n_rows, ncols=n_cols)\n",
    "    for n in range (n_plot):    \n",
    "        img_id = image_ids[n]        \n",
    "        img = images[n].numpy()\n",
    "        img = np.squeeze(img)\n",
    "        gt_target = gt_targets[n]\n",
    "        boxes = gt_target['boxes'].numpy().astype(np.int32)\n",
    "        labels = gt_target['labels'].numpy().astype(np.int32)\n",
    "        gt_anns = [(l, b) for l, b in zip(labels, boxes) if l > 0]    \n",
    "        pred_anns = None\n",
    "\n",
    "        row = n // n_cols\n",
    "        col = n % n_cols \n",
    "        \n",
    "        if n_rows == 1:\n",
    "            sub_ax = ax[col]\n",
    "        else:\n",
    "            sub_ax = ax[row][col]\n",
    "        if pred_targets is not None:\n",
    "            pred_target = pred_targets[n]\n",
    "            boxes = pred_target['boxes'].numpy().astype(np.int32)\n",
    "            scores = pred_target['scores'].numpy()\n",
    "            #labels = pred_target['labels'].numpy().astype(np.int32)\n",
    "            pred_anns = [(f\"{s:.2f}\", b) for s, b in zip(scores, boxes) if s > 0]              \n",
    "        \n",
    "        show_img_with_boxes(img, gt_anno=gt_anns, pred_anno=pred_anns, ax=sub_ax, title=img_id, box_format='xyxy')                   \n",
    "\n",
    "    plt.tight_layout()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('../data/siim-covid19-detection-subset')\n",
    "train_path = data_path / \"train\"\n",
    "\n",
    "# annotation data frame\n",
    "ann_df = pd.read_csv(data_path / \"train_annotations.csv\", converters={\n",
    "    \"boxes\": literal_eval, \n",
    "    \"labels\": literal_eval,\n",
    "    \"pixel_spacing\": literal_eval\n",
    "   })  \n",
    "\n",
    "ann_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fix for situations like this. Positive but no box showing opacity\n",
    "ann_df[ann_df[\"id\"] == '0bd6cd815ba9_image']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-21T11:21:33.733067Z",
     "iopub.status.busy": "2021-05-21T11:21:33.732798Z",
     "iopub.status.idle": "2021-05-21T11:21:33.740040Z",
     "shell.execute_reply": "2021-05-21T11:21:33.738984Z",
     "shell.execute_reply.started": "2021-05-21T11:21:33.733038Z"
    }
   },
   "outputs": [],
   "source": [
    "# normalization transforms\n",
    "norm_transform_list = [\n",
    "    A.Resize(height=Config.img_size[0], width=Config.img_size[1], p=1.0),\n",
    "    A.Normalize(mean=(0,), std=(1,), p=1.0),\n",
    "    ToTensorV2(p=1.0)    \n",
    "]\n",
    "\n",
    "# augmentation transforms\n",
    "aug_transform_list = [\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2,  \n",
    "                               contrast_limit=0.2, p=0.5),\n",
    "    A.CropAndPad(percent=(0.0, 0.02), pad_mode=cv2.BORDER_CONSTANT, \n",
    "                 pad_cval=0, keep_size=True, sample_independently=True, p=0.5)\n",
    "]\n",
    "\n",
    "# bounding box format\n",
    "bbox_params = {'format': 'pascal_voc', 'label_fields': ['labels']}\n",
    "\n",
    "train_transforms = A.Compose(aug_transform_list + norm_transform_list, bbox_params=bbox_params)\n",
    "val_transforms = A.Compose(norm_transform_list, bbox_params=bbox_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-21T11:21:33.758802Z",
     "iopub.status.busy": "2021-05-21T11:21:33.758432Z",
     "iopub.status.idle": "2021-05-21T11:21:33.773130Z",
     "shell.execute_reply": "2021-05-21T11:21:33.772324Z",
     "shell.execute_reply.started": "2021-05-21T11:21:33.758766Z"
    }
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "\n",
    "train_df = ann_df[ann_df['fold'] != Config.fold_num]\n",
    "val_df = ann_df[ann_df['fold'] == Config.fold_num]\n",
    "\n",
    "train_ds = DetectionDataset(train_df, train_path, train_transforms)\n",
    "val_ds = DetectionDataset(val_df, train_path, val_transforms)\n",
    "\n",
    "train_dl = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size = Config.batch_size,\n",
    "        shuffle = True,\n",
    "        num_workers = Config.num_workers,\n",
    "        collate_fn = collate_fn\n",
    "    )\n",
    "val_dl = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size = Config.batch_size,\n",
    "        shuffle = False,\n",
    "        num_workers = Config.num_workers,    \n",
    "        collate_fn = collate_fn    \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-21T11:21:33.774773Z",
     "iopub.status.busy": "2021-05-21T11:21:33.774430Z",
     "iopub.status.idle": "2021-05-21T11:21:33.948894Z",
     "shell.execute_reply": "2021-05-21T11:21:33.948049Z",
     "shell.execute_reply.started": "2021-05-21T11:21:33.774739Z"
    }
   },
   "outputs": [],
   "source": [
    "image, target, image_id = train_ds[307]\n",
    "\n",
    "img_np = image.numpy()\n",
    "img_np = np.squeeze(img_np)\n",
    "boxes = target['boxes'].numpy().astype(np.int32)\n",
    "labels = target['labels'].numpy().astype(np.int32)\n",
    "gt_anns = [(l, b) for l, b in zip(labels, boxes) if l > 0]\n",
    "\n",
    "_ = show_img_with_boxes(img_np, gt_anno=gt_anns, figsize=(12, 12), title=image_id, box_format='xyxy')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_it = iter(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-21T11:21:33.959547Z",
     "iopub.status.busy": "2021-05-21T11:21:33.958937Z",
     "iopub.status.idle": "2021-05-21T11:21:38.481005Z",
     "shell.execute_reply": "2021-05-21T11:21:38.480021Z",
     "shell.execute_reply.started": "2021-05-21T11:21:33.959507Z"
    }
   },
   "outputs": [],
   "source": [
    "images, targets, image_ids = next(train_it)\n",
    "plot_image_batch(images, targets, image_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-21T11:21:38.592357Z",
     "iopub.status.busy": "2021-05-21T11:21:38.592040Z",
     "iopub.status.idle": "2021-05-21T11:21:39.369772Z",
     "shell.execute_reply": "2021-05-21T11:21:39.368876Z",
     "shell.execute_reply.started": "2021-05-21T11:21:38.592319Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_model(cfg, checkpoint_path=None):\n",
    "    model = DetectionBaseline(cfg)    \n",
    "    \n",
    "    # Load the trained weights\n",
    "    if checkpoint_path is not None:\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "        del checkpoint\n",
    "        gc.collect()\n",
    "        \n",
    "    return model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(Config)\n",
    "model.to(Config.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, target, image_id = train_ds[0]\n",
    "img_np = image.numpy()\n",
    "img_np = np.squeeze(img_np)\n",
    "\n",
    "anchors = model.anchors\n",
    "anchors_np = anchors.cpu().numpy()\n",
    "anchors_np = cxcywh_to_xyxy(anchors_np)\n",
    "anchors_np = np.clip(anchors_np, 0.0, 1.0)\n",
    "anchors_np = boxes_xyxy_rel_to_abs(anchors_np, img_np.shape)\n",
    "\n",
    "print(\"Number of anchors:\", anchors.shape[0])\n",
    "print(\"Number of cells in last feature layer:\", (Config.img_size[0] // 32)**2)\n",
    "\n",
    "n_anchor_plot = 8 # how many anchors to plot (all would be too messy)\n",
    "anchor_plot_idx = np.round(np.linspace(0, len(anchors_np) - 1, n_anchor_plot)).astype(int)\n",
    "anchor_anns = [(i, b) for i, b in enumerate(anchors_np[anchor_plot_idx])]\n",
    "\n",
    "_ = show_img_with_boxes(img_np, gt_anno=anchor_anns, figsize=(12, 12), title=image_id, box_format='xyxy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-21T11:21:38.592357Z",
     "iopub.status.busy": "2021-05-21T11:21:38.592040Z",
     "iopub.status.idle": "2021-05-21T11:21:39.369772Z",
     "shell.execute_reply": "2021-05-21T11:21:39.368876Z",
     "shell.execute_reply.started": "2021-05-21T11:21:38.592319Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer = DetectionTrainer(model, Config)\n",
    "trainer.fit(train_dl, val_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-21T11:21:40.190243Z",
     "iopub.status.busy": "2021-05-21T11:21:40.189964Z",
     "iopub.status.idle": "2021-05-21T11:21:41.276719Z",
     "shell.execute_reply": "2021-05-21T11:21:41.275880Z",
     "shell.execute_reply.started": "2021-05-21T11:21:40.190212Z"
    }
   },
   "outputs": [],
   "source": [
    "model_path = Config.log_path / 'last-checkpoint.pt'\n",
    "#model_path = Path(\"./logs/\") / \"detection\" / \"ssd_2021-07-09_21-22-36\" / 'best-checkpoint-092epoch.pt'\n",
    "\n",
    "model = get_model(Config, checkpoint_path=model_path)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_it = iter(val_dl)\n",
    "\n",
    "pred_threshold = 0.5\n",
    "nms_threshold = 0.2 # max overlap allowed for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-21T11:21:39.417113Z",
     "iopub.status.busy": "2021-05-21T11:21:39.416817Z",
     "iopub.status.idle": "2021-05-21T11:21:40.185922Z",
     "shell.execute_reply": "2021-05-21T11:21:40.184909Z",
     "shell.execute_reply.started": "2021-05-21T11:21:39.417088Z"
    }
   },
   "outputs": [],
   "source": [
    "images, gt_targets, image_ids = next(val_it)\n",
    "images = torch.stack(images)\n",
    "images = images.to(Config.device)\n",
    "\n",
    "with torch.no_grad():        \n",
    "    pred_locs, pred_scores = model(images)       \n",
    "    det_boxes_batch, det_labels_batch, det_scores_batch = model.detect_objects(pred_locs, pred_scores,\n",
    "                                                                               min_score=pred_threshold, \n",
    "                                                                               max_overlap=nms_threshold,\n",
    "                                                                               top_k=100)  \n",
    "    det_boxes_batch = [torch.clip(b, 0, 1) for b in det_boxes_batch]\n",
    "    det_boxes_batch = [boxes_xyxy_rel_to_abs(b, img.shape[1:]) for b, img in zip(det_boxes_batch, images)]    \n",
    "\n",
    "images = images.cpu()    \n",
    "    \n",
    "pred_targets = []    \n",
    "for b, l, s in zip(det_boxes_batch, det_labels_batch, det_scores_batch):\n",
    "    pred_target = {\n",
    "        'boxes' : b.cpu(),\n",
    "        'labels' : l.cpu(),\n",
    "        'scores' : s.cpu()        \n",
    "    }\n",
    "    pred_targets.append(pred_target)\n",
    "\n",
    "plot_image_batch(images, gt_targets, image_ids, pred_targets=pred_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det_boxes = list()\n",
    "det_labels = list()\n",
    "det_scores = list()\n",
    "true_boxes = list()\n",
    "true_labels = list()\n",
    "\n",
    "for i, (images, targets, image_ids) in enumerate(val_dl):\n",
    "    images = torch.stack(images)\n",
    "    images = images.to(Config.device)\n",
    "    \n",
    "    boxes = [boxes_xyxy_abs_to_rel(t['boxes'].to(torch.float).to(\n",
    "        Config.device), img.shape[1:]) for t, img in zip(targets, images)]\n",
    "    labels = [t['labels'].to(Config.device) for t in targets]    \n",
    "\n",
    "    with torch.no_grad():        \n",
    "        pred_locs, pred_scores = model(images)     \n",
    "    \n",
    "    det_boxes_batch, det_labels_batch, det_scores_batch = model.detect_objects(pred_locs, pred_scores,\n",
    "                                                                               min_score=0.01, \n",
    "                                                                               max_overlap=0.45,\n",
    "                                                                               top_k=200)  \n",
    "    \n",
    "    det_boxes.extend(det_boxes_batch)\n",
    "    det_labels.extend(det_labels_batch)\n",
    "    det_scores.extend(det_scores_batch)\n",
    "    true_boxes.extend(boxes)\n",
    "    true_labels.extend(labels)    \n",
    "\n",
    "APs, mAP = calculate_mAP(det_boxes, det_labels, det_scores, true_boxes, true_labels, Config)\n",
    "\n",
    "print(\"Average precision per class:\")\n",
    "for k,v in APs.items():\n",
    "    print(f\"{k} \\t {v:.5f}\")\n",
    "print(f\"Mean average precision: {mAP:.5f}\")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
